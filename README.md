# Image-Captioning-Using-CNN-and-LSTM

This project implements an image captioning model using PyTorch, based on the Flickr8k dataset. The model uses a Convolutional Neural Network (CNN) as an encoder to extract image features and a Recurrent Neural Network (RNN) as a decoder to generate descriptive captions.

## Table of Contents

- Model Architecture
- Dataset
- Results
- Setup and Installation
- Usage
- Directory Structure

## Model Architecture

The model follows a standard encoder-decoder architecture, which is a common and effective approach for image captioning.

- **Encoder**: A pre-trained **ResNet-18** model acts as the encoder. We remove the final classification layer and use the output of the convolutional base as a rich feature vector that represents the input image's content.
- **Decoder**: A **Long Short-Term Memory (LSTM)** network serves as the decoder. It takes the image feature vector from the encoder and generates the caption word by word.

The process is as follows:

1. The input image is passed through the ResNet-18 encoder to get a fixed-size feature vector.
2. This feature vector is fed as the initial hidden state to the LSTM decoder.
3. The decoder generates the caption sequentially, starting with a &lt;SOS&gt; (start of sentence) token and predicting the next word until an &lt;EOS&gt; (end of sentence) token is produced.

## Dataset

This project uses the **Flickr8k dataset**, which contains:

- 8,091 images
- 5 captions for each image

**Note:** The dataset is not included in this repository due to its size.

You can download the dataset from [this Kaggle page](https://www.kaggle.com/datasets/adityajn105/flickr8k). You will need the images folder and the captions.txt file.

## Results

The project conducts two main experiments to evaluate the best training strategy:

1. **Training with a Frozen Encoder**: In this experiment, the weights of the pre-trained ResNet-18 are frozen. Only the final linear layer of the encoder and the LSTM decoder are trained. This approach leverages the powerful, general-purpose features learned by ResNet-18 on the ImageNet dataset.
2. **Training with an Unfrozen (Fine-Tuned) Encoder**: Here, all layers of the ResNet-18 are unfrozen and trained along with the decoder. This allows the encoder to adapt its features specifically to the Flickr8k dataset.

### Loss Comparison

As shown in the plots below, the model with the **frozen encoder** achieved better results. It showed a stable decrease in both training and test loss. The fine-tuning approach led to overfitting, where the test loss started to increase while the training loss decreased.

| **Training with Frozen ResNet-18** | **Training with Unfrozen ResNet-18** |
| --- | --- |
|     |     |
| --- | --- |

**Conclusion**: For this dataset, freezing the pre-trained encoder layers is the more effective strategy. It prevents overfitting and produces more coherent captions.

### Sample Predictions (Frozen Model)

Here are a few examples of captions generated by the final model (with the frozen encoder):

| **Image** | **Generated Caption** |
| --- | --- |
|     | a dog is running through the snow |
| --- | --- |
|     | a little boy is playing with a toy car |
| --- | --- |
|     | a man in a red shirt is standing on a rock |
| --- | --- |

## Setup and Installation

Follow these steps to set up the project environment.

**1\. Clone the repository:**

git clone \[<https://github.com/YOUR_USERNAME/Image-Captioning-Flickr8k.git\>](<https://github.com/YOUR_USERNAME/Image-Captioning-Flickr8k.git>)  
cd Image-Captioning-Flickr8k  

**2\. Create a virtual environment (recommended):**

python -m venv venv  
source venv/bin/activate # On Windows, use \`venv\\Scripts\\activate\`  

**3\. Install the required libraries:**

pip install -r requirements.txt  

**4\. Download the Dataset:**

- Download the Flickr8k dataset from [Kaggle](https://www.kaggle.com/datasets/adityajn105/flickr8k).
- Create a data folder in the project's root directory.
- Unzip the downloaded file and place the images folder and captions.txt file inside the data folder.

## Usage

Once the setup is complete, you can run the model by launching the Jupyter Notebook:

jupyter notebook Image_Captioning.ipynb  

The notebook contains all the steps from data preprocessing and model definition to training and evaluation. You can run the cells sequentially to reproduce the results.

## Directory Structure

The project should be organized as follows for the code to run correctly:

Image-Captioning-Flickr8k/  
├── data/  
│ ├── images/  
│ │ ├── 1000268201_693b08cb0e.jpg  
│ │ └── ... (all other images)  
│ └── captions.txt  
├── Image_Captioning.ipynb  
├── README.md  
└── requirements.txt
